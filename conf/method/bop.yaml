method: bayes-over-prompt

defaults:
- mcmc: malangevin

# MCMC parameters
steps: 20
burn_in: 2
thinning: 2
num_chains: 1
num_repeats: 1  # Number of times to repeat each sampled model

# The LLM-based system for solving the QA problem
model:
  _partial_: true
  _target_: method.models.SimpleModel
  engine: gpt-4o-mini  # change to gpt-4o for SimpleQA

# An Optimizer partial that, once instantiated, takes in model parameters
optimizer:
  _partial_: true
  _target_: method.tgext.optimizer.TextualGradientDescentLogProb
  constraints:
    - The system prompt must be at most one short paragraph.
    - The system prompt must be generic and applicable to any question.
    - The system prompt must not include formatting instructions for the last line of the response since they will be provided separately.
  verbose: false
  engine:
    _target_: method.tgext.together.ChatTogetherLogProb
    model_string: nvidia/Llama-3.1-Nemotron-70B-Instruct-HF

# A Proposal partial that, once instantiated, takes in a model and an optimizer
proposal:
  _partial_: true
  _target_: method.mcmc.proposals.TextGradProposal
  likelihood_loss:
    _target_: method.losses.LikelihoodLoss
    evaluation_instruction: ${data.likelihood_loss_text}
    role_description: ${data.likelihood_loss_role},
    input_roles: ${data.likelihood_loss_input_roles}
    engine: gpt-4o

chain_kwargs:
  # Args to be passed into the model during MCMC sampling
  temperature: 1
  top_p: 1.0

eval_kwargs:
  num_examples: 100  # Number of examples to evaluate on
  num_processes: 1 # Number of evaluation examples to run in parallel

  # Args to be passed into the model at inference time
  temperature: 1
  top_p: 1.0
